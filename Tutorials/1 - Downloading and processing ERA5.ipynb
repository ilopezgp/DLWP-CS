{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Downloading and processing ERA5 data\n",
    "\n",
    "In this tutorial, we will use the DLWP data module to fetch and pre-process data from ERA5 to use in a DLWP weather prediction model. For the sake of simplicity, we use only a select few variables over a few years.\n",
    "\n",
    "#### Python packages required here not in the base requirements\n",
    "\n",
    "Let's start by installing the `cdsapi` package, which is required for retrieval of data. (See the README for packages already required for DLWP that need to also be installed.) Note that to use `cdsapi` you will need to register for an API key at CDS, following [their instructions](https://cds.climate.copernicus.eu/api-how-to)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting package metadata (current_repodata.json): done\n",
      "Solving environment: failed with initial frozen solve. Retrying with flexible solve.\n",
      "Solving environment: failed with repodata from current_repodata.json, will retry with next repodata source.\n",
      "Collecting package metadata (repodata.json): done\n",
      "Solving environment: - ^C\n",
      "/ \n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%conda install -c conda-forge cdsapi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieve data\n",
    "\n",
    "Define the variables and levels we want to retrieve. Single-level variables ignore the \"levels\" parameter. Also note that not all variables in the ERA5 dataset are coded with their parameter names as of now. We also take a reduced sample of years in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "variables = ['geopotential']\n",
    "levels = [500]\n",
    "years = list(range(2013, 2019))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize the data retriever. You'll want to change the directory to where you want to save the files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir(os.pardir)\n",
    "from DLWP.data import ERA5Reanalysis\n",
    "\n",
    "data_directory = '/usr/local/google/ilopezgp/ERA5_data_dlwp'\n",
    "os.makedirs(data_directory, exist_ok=True)\n",
    "era = ERA5Reanalysis(root_directory=data_directory, file_id='tutorial')\n",
    "era.set_variables(variables)\n",
    "era.set_levels(levels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download data! Automatically uses multi-processing to retrieve multiple files at a time. Note the parameter `hourly` says we're retrieving only every 3rd hour in the data, which is available hourly. The optional parameter passed to the retrieval package specifies that we want data interpolated to a 2-by-2 latitude-longitude grid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ERA5Reanalysis.retrieve: WARNING: file /usr/local/google/ilopezgp/ERA5_data_dlwp/tutorial_geopotential_500.nc already exists; omitting\n"
     ]
    }
   ],
   "source": [
    "era.retrieve(variables, levels, years=years, hourly=3,\n",
    "             request_kwargs={'grid': [2., 2.]}, verbose=True, delete_temporary=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check that we got what we wanted after the retrieval is done:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<xarray.Dataset>\n",
      "Dimensions:    (latitude: 91, level: 1, longitude: 180, time: 17528)\n",
      "Coordinates:\n",
      "  * longitude  (longitude) float32 0.0 2.0 4.0 6.0 ... 352.0 354.0 356.0 358.0\n",
      "  * latitude   (latitude) float32 90.0 88.0 86.0 84.0 ... -86.0 -88.0 -90.0\n",
      "  * time       (time) datetime64[ns] 2013-01-01 ... 2018-12-31T21:00:00\n",
      "  * level      (level) float32 500.0\n",
      "Data variables:\n",
      "    z          (time, level, latitude, longitude) float32 dask.array<chunksize=(17528, 1, 91, 180), meta=np.ndarray>\n",
      "Attributes:\n",
      "    Conventions:  CF-1.6\n",
      "    history:      2021-07-02 18:23:46 GMT by grib_to_netcdf-2.20.0: /opt/ecmw...\n"
     ]
    }
   ],
   "source": [
    "era.open()\n",
    "print(era.Dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process data for ingestion into DLWP\n",
    "\n",
    "Now we use the DLWP.model.Preprocessor tool to generate a new data file ready for use in a DLWP Keras model. Some preliminaries... Note that we assign level \"0\" to the single-level 2m temperature data. I highly recommend using \"pairwise\" data processing, which means that each variable is matched to a level pair-wise. The length of the variables and levels lists should be the same. Also note that you only need to specify whole days in the dates. It takes care of the hourly data automatically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from DLWP.data.era5 import get_short_name\n",
    "\n",
    "dates = list(pd.date_range('2013-01-01', '2018-12-31', freq='D').to_pydatetime())\n",
    "variables = ['z'] #get_short_name(variables)\n",
    "levels = [500]\n",
    "processed_file = '%s/tutorial_z500.nc' % data_directory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Process data! For proper use of data in a neural network, variables must be normalized relative to each other. This is typically done simply by removing mean and dividing by standard deviation (`scale_variables` option). To save on memory use, we normally calculate the global mean and std of the data in batches. Since this is a small dataset, we can use a large batch size to make it go faster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessor.data_to_samples: opening and formatting raw data\n",
      "Preprocessor.data_to_samples: creating output file /usr/local/google/ilopezgp/ERA5_data_dlwp/tutorial_z500.nc\n",
      "Preprocessor.data_to_samples: variable/level pair 1 of 1 (z/500)\n",
      "Preprocessor.data_to_samples: calculating mean and std\n",
      "Preprocessor.data_to_samples: writing batch 1 of 2\n",
      "Preprocessor.data_to_samples: writing batch 2 of 2\n"
     ]
    }
   ],
   "source": [
    "from DLWP.model import Preprocessor\n",
    "\n",
    "pp = Preprocessor(era, predictor_file=processed_file)\n",
    "pp.data_to_series(batch_samples=10000, variables=variables, levels=levels, pairwise=True,\n",
    "                  scale_variables=True, overwrite=True, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show our dataset, then clean up. We also save to a version with no string coordinates (might be needed for tempest-remap in the next tutorial)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<xarray.Dataset>\n",
      "Dimensions:     (lat: 91, lon: 180, sample: 17528, varlev: 1)\n",
      "Coordinates:\n",
      "  * lat         (lat) float32 90.0 88.0 86.0 84.0 ... -84.0 -86.0 -88.0 -90.0\n",
      "  * lon         (lon) float32 0.0 2.0 4.0 6.0 8.0 ... 352.0 354.0 356.0 358.0\n",
      "  * varlev      (varlev) object 'z/500'\n",
      "  * sample      (sample) datetime64[ns] 2013-01-01 ... 2018-12-31T21:00:00\n",
      "Data variables:\n",
      "    predictors  (sample, varlev, lat, lon) float32 dask.array<chunksize=(1, 1, 91, 180), meta=np.ndarray>\n",
      "    mean        (varlev) float32 dask.array<chunksize=(1,), meta=np.ndarray>\n",
      "    std         (varlev) float32 dask.array<chunksize=(1,), meta=np.ndarray>\n",
      "Attributes:\n",
      "    description:  Training data for DLWP\n",
      "    scaling:      True\n"
     ]
    }
   ],
   "source": [
    "print(pp.data)\n",
    "pp.data.drop('varlev').to_netcdf(processed_file + '.nocoord')\n",
    "era.close()\n",
    "pp.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
